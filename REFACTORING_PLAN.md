# üìã –î–ï–¢–ê–õ–¨–ù–ï–ô–®–ò–ô –ü–õ–ê–ù –†–ï–§–ê–ö–¢–û–†–ò–ù–ì–ê: Azerbaijan Press Monitor Multi-Agent System

## üéØ –¶–ï–õ–¨ –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò
–ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π research agent –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–∏—Ä–æ–≤–æ–π –ø—Ä–µ—Å—Å—ã –æ–± –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.

## üåç –ì–õ–û–ë–ê–õ–¨–ù–´–ô –û–•–í–ê–¢ –° –ü–†–ò–û–†–ò–¢–ï–¢–û–ú –ù–ê –ê–ó–ò–Æ –ò –°–û–°–ï–î–ï–ô

### üéØ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –†–ï–ì–ò–û–ù–´:
1. **–°–û–°–ï–î–ò –ê–ó–ï–†–ë–ê–ô–î–ñ–ê–ù–ê** (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û):
   - üáπüá∑ –¢—É—Ä—Ü–∏—è (—Ç—É—Ä–µ—Ü–∫–∏–π) - –∫–ª—é—á–µ–≤–æ–π –ø–∞—Ä—Ç–Ω–µ—Ä
   - üá∑üá∫ –†–æ—Å—Å–∏—è (—Ä—É—Å—Å–∫–∏–π) - –≤–∞–∂–Ω–µ–π—à–∏–π —Å–æ—Å–µ–¥
   - üáÆüá∑ –ò—Ä–∞–Ω (–ø–µ—Ä—Å–∏–¥—Å–∫–∏–π/—Ñ–∞—Ä—Å–∏) - —é–∂–Ω—ã–π —Å–æ—Å–µ–¥
   - üá¨üá™ –ì—Ä—É–∑–∏—è (–≥—Ä—É–∑–∏–Ω—Å–∫–∏–π) - —Å–µ–≤–µ—Ä–Ω—ã–π —Å–æ—Å–µ–¥
   - üá¶üá≤ –ê—Ä–º–µ–Ω–∏—è (–∞—Ä–º—è–Ω—Å–∫–∏–π) - –∑–∞–ø–∞–¥–Ω—ã–π —Å–æ—Å–µ–¥

2. **–°–†–ï–î–ù–Ø–Ø –ê–ó–ò–Ø** (–í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢):
   - üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω (–∫–∞–∑–∞—Ö—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π)
   - üá∫üáø –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω (—É–∑–±–µ–∫—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π)
   - üáπüá≤ –¢—É—Ä–∫–º–µ–Ω–∏—Å—Ç–∞–Ω (—Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π)
   - üá∞üá¨ –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω (–∫—ã—Ä–≥—ã–∑—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π)
   - üáπüáØ –¢–∞–¥–∂–∏–∫–∏—Å—Ç–∞–Ω (—Ç–∞–¥–∂–∏–∫—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π)

3. **–Æ–ì–û-–í–û–°–¢–û–ß–ù–ê–Ø –ê–ó–ò–Ø**:
   - üáπüá≠ –¢–∞–∏–ª–∞–Ω–¥ (—Ç–∞–π—Å–∫–∏–π)
   - üáÆüá© –ò–Ω–¥–æ–Ω–µ–∑–∏—è (–∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏–π)
   - üá≤üáæ –ú–∞–ª–∞–π–∑–∏—è (–º–∞–ª–∞–π—Å–∫–∏–π)
   - üáµüá≠ –§–∏–ª–∏–ø–ø–∏–Ω—ã (—Ç–∞–≥–∞–ª–æ–≥, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
   - üáªüá≥ –í—å–µ—Ç–Ω–∞–º (–≤—å–µ—Ç–Ω–∞–º—Å–∫–∏–π)
   - üá∏üá¨ –°–∏–Ω–≥–∞–ø—É—Ä (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –∫–∏—Ç–∞–π—Å–∫–∏–π, –º–∞–ª–∞–π—Å–∫–∏–π)

4. **–í–û–°–¢–û–ß–ù–ê–Ø –ê–ó–ò–Ø**:
   - üá®üá≥ –ö–∏—Ç–∞–π (–∫–∏—Ç–∞–π—Å–∫–∏–π –º–∞–Ω–¥–∞—Ä–∏–Ω)
   - üáØüáµ –Ø–ø–æ–Ω–∏—è (—è–ø–æ–Ω—Å–∫–∏–π)
   - üá∞üá∑ –Æ–∂–Ω–∞—è –ö–æ—Ä–µ—è (–∫–æ—Ä–µ–π—Å–∫–∏–π)
   - üá≤üá≥ –ú–æ–Ω–≥–æ–ª–∏—è (–º–æ–Ω–≥–æ–ª—å—Å–∫–∏–π)

5. **–Æ–ñ–ù–ê–Ø –ê–ó–ò–Ø**:
   - üáÆüá≥ –ò–Ω–¥–∏—è (—Ö–∏–Ω–¥–∏, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏)
   - üáµüá∞ –ü–∞–∫–∏—Å—Ç–∞–Ω (—É—Ä–¥—É, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
   - üáßüá© –ë–∞–Ω–≥–ª–∞–¥–µ—à (–±–µ–Ω–≥–∞–ª—å—Å–∫–∏–π)
   - üá±üá∞ –®—Ä–∏-–õ–∞–Ω–∫–∞ (—Å–∏–Ω–≥–∞–ª—å—Å–∫–∏–π, —Ç–∞–º–∏–ª—å—Å–∫–∏–π)

6. **–ï–í–†–û–ü–ê** (–í–ê–ñ–ù–û –î–õ–Ø –ü–û–õ–ù–û–ô –ö–ê–†–¢–ò–ù–´):
   - üá¨üáß –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
   - üá©üá™ –ì–µ—Ä–º–∞–Ω–∏—è (–Ω–µ–º–µ—Ü–∫–∏–π)
   - üá´üá∑ –§—Ä–∞–Ω—Ü–∏—è (—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π)
   - üáÆüáπ –ò—Ç–∞–ª–∏—è (–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π)
   - üá™üá∏ –ò—Å–ø–∞–Ω–∏—è (–∏—Å–ø–∞–Ω—Å–∫–∏–π)
   - üáµüáπ –ü–æ—Ä—Ç—É–≥–∞–ª–∏—è (–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π)
   - üáµüá± –ü–æ–ª—å—à–∞ (–ø–æ–ª—å—Å–∫–∏–π)
   - üá∫üá¶ –£–∫—Ä–∞–∏–Ω–∞ (—É–∫—Ä–∞–∏–Ω—Å–∫–∏–π)
   - üá∏üá™ –®–≤–µ—Ü–∏—è (—à–≤–µ–¥—Å–∫–∏–π)
   - üá≥üá¥ –ù–æ—Ä–≤–µ–≥–∏—è (–Ω–æ—Ä–≤–µ–∂—Å–∫–∏–π)
   - üá´üáÆ –§–∏–Ω–ª—è–Ω–¥–∏—è (—Ñ–∏–Ω—Å–∫–∏–π)
   - üáÆüá∏ –ò—Å–ª–∞–Ω–¥–∏—è (–∏—Å–ª–∞–Ω–¥—Å–∫–∏–π)
   - üá¨üá∑ –ì—Ä–µ—Ü–∏—è (–≥—Ä–µ—á–µ—Å–∫–∏–π)
   - –í–°–ï –æ—Å—Ç–∞–ª—å–Ω—ã–µ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω—ã

7. **–ê–§–†–ò–ö–ê** (–í–ê–ñ–ù–û –î–õ–Ø –ü–û–õ–ù–û–ô –ö–ê–†–¢–ò–ù–´):
   - üá™üá¨ –ï–≥–∏–ø–µ—Ç (–∞—Ä–∞–±—Å–∫–∏–π)
   - üáøüá¶ –Æ–ê–† (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –∞—Ñ—Ä–∏–∫–∞–∞–Ω—Å, –∑—É–ª—É)
   - üá≥üá¨ –ù–∏–≥–µ—Ä–∏—è (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –π–æ—Ä—É–±–∞)
   - üá∞üá™ –ö–µ–Ω–∏—è (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Å—É–∞—Ö–∏–ª–∏)
   - üá™üáπ –≠—Ñ–∏–æ–ø–∏—è (–∞–º—Ö–∞—Ä—Å–∫–∏–π)
   - üá≤üá¶ –ú–∞—Ä–æ–∫–∫–æ (–∞—Ä–∞–±—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π)
   - üá©üáø –ê–ª–∂–∏—Ä (–∞—Ä–∞–±—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π)
   - –í–°–ï –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω—ã

8. **–ê–ú–ï–†–ò–ö–ê** (–í–ê–ñ–ù–û –î–õ–Ø –ü–û–õ–ù–û–ô –ö–ê–†–¢–ò–ù–´):
   - üá∫üá∏ –°–®–ê (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
   - üá®üá¶ –ö–∞–Ω–∞–¥–∞ (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π)
   - üá≤üáΩ –ú–µ–∫—Å–∏–∫–∞ (–∏—Å–ø–∞–Ω—Å–∫–∏–π)
   - üáßüá∑ –ë—Ä–∞–∑–∏–ª–∏—è (–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π)
   - üá¶üá∑ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–∞ (–∏—Å–ø–∞–Ω—Å–∫–∏–π)
   - üá®üá¥ –ö–æ–ª—É–º–±–∏—è (–∏—Å–ø–∞–Ω—Å–∫–∏–π)
   - üáµüá™ –ü–µ—Ä—É (–∏—Å–ø–∞–Ω—Å–∫–∏–π, –∫–µ—á—É–∞)
   - üá®üá± –ß–∏–ª–∏ (–∏—Å–ø–∞–Ω—Å–∫–∏–π)
   - –í–°–ï —Å—Ç—Ä–∞–Ω—ã –õ–∞—Ç–∏–Ω—Å–∫–æ–π –ê–º–µ—Ä–∏–∫–∏

9. **–û–ö–ï–ê–ù–ò–Ø** (–í–ê–ñ–ù–û –î–õ–Ø –ü–û–õ–ù–û–ô –ö–ê–†–¢–ò–ù–´):
   - üá¶üá∫ –ê–≤—Å—Ç—Ä–∞–ª–∏—è (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
   - üá≥üáø –ù–æ–≤–∞—è –ó–µ–ª–∞–Ω–¥–∏—è (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –º–∞–æ—Ä–∏)
   - üáµüá¨ –ü–∞–ø—É–∞-–ù–æ–≤–∞—è –ì–≤–∏–Ω–µ—è
   - üá´üáØ –§–∏–¥–∂–∏
   - –í–°–ï –æ—Å—Ç—Ä–æ–≤–Ω—ã–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞

10. **–ë–õ–ò–ñ–ù–ò–ô –í–û–°–¢–û–ö** (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û):
    - üá∏üá¶ –°–∞—É–¥–æ–≤—Å–∫–∞—è –ê—Ä–∞–≤–∏—è (–∞—Ä–∞–±—Å–∫–∏–π)
    - üá¶üá™ –û–ê–≠ (–∞—Ä–∞–±—Å–∫–∏–π)
    - üáÆüá± –ò–∑—Ä–∞–∏–ª—å (–∏–≤—Ä–∏—Ç, –∞—Ä–∞–±—Å–∫–∏–π)
    - üá∂üá¶ –ö–∞—Ç–∞—Ä (–∞—Ä–∞–±—Å–∫–∏–π)
    - üá∞üáº –ö—É–≤–µ–π—Ç (–∞—Ä–∞–±—Å–∫–∏–π)
    - üáØüá¥ –ò–æ—Ä–¥–∞–Ω–∏—è (–∞—Ä–∞–±—Å–∫–∏–π)
    - üá±üáß –õ–∏–≤–∞–Ω (–∞—Ä–∞–±—Å–∫–∏–π)
    - üá∏üáæ –°–∏—Ä–∏—è (–∞—Ä–∞–±—Å–∫–∏–π)
    - üáÆüá∂ –ò—Ä–∞–∫ (–∞—Ä–∞–±—Å–∫–∏–π, –∫—É—Ä–¥—Å–∫–∏–π)

**–í–ê–ñ–ù–û**: –ö–ê–ñ–î–ê–Ø –°–¢–†–ê–ù–ê –ò –ö–ê–ñ–î–´–ô –Ø–ó–´–ö –í–ê–ñ–ù–´ –î–õ–Ø –ü–û–õ–ù–û–ì–û –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø!

## üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ô –ü–û–î–•–û–î

### –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞:
1. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ –∫–æ–¥–∞** - –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ
2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã** - LangGraph, Redis, PostgreSQL
3. **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –∞ –Ω–µ –∑–∞–º–µ–Ω–∞** - –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —É–∑–ª—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –≥—Ä–∞—Ñ—É
4. **–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** - —Å—Ç–∞—Ä—ã–π research flow –æ—Å—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω—ã–º

## üìä –§–ê–ó–ê 1: –†–ê–°–®–ò–†–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• (–î–µ–Ω—å 1-2)

### 1.1 –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã PostgreSQL

```sql
-- –ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
CREATE TABLE IF NOT EXISTS press_articles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    source_name TEXT NOT NULL,
    source_country TEXT,
    source_language VARCHAR(10) NOT NULL,
    language_name TEXT NOT NULL, -- –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞
    region TEXT, -- –∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç/—Ä–µ–≥–∏–æ–Ω
    published_date TIMESTAMP WITH TIME ZONE,
    fetched_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    original_content TEXT, -- –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    translated_content TEXT, -- –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
    summary TEXT,
    sentiment VARCHAR(20) CHECK (sentiment IN ('positive', 'negative', 'neutral')),
    sentiment_score FLOAT,
    sentiment_explanation TEXT, -- –ø–æ—á–µ–º—É —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞
    key_phrases JSONB,
    mentions_context JSONB, -- –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞
    thread_id UUID,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS press_digests (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    digest_type VARCHAR(20) CHECK (digest_type IN ('positive', 'negative', 'daily', 'weekly')),
    content TEXT NOT NULL,
    articles_count INTEGER,
    languages_covered JSONB, -- {"en": 5, "ru": 3, "tr": 2, ...}
    countries_covered JSONB,
    regions_breakdown JSONB, -- {"Europe": 10, "Asia": 15, ...}
    generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    sent_email BOOLEAN DEFAULT FALSE,
    sent_telegram BOOLEAN DEFAULT FALSE
);

CREATE TABLE IF NOT EXISTS language_coverage (
    language_code VARCHAR(10) PRIMARY KEY,
    language_name TEXT NOT NULL,
    native_name TEXT,
    region TEXT,
    countries TEXT[],
    last_checked TIMESTAMP WITH TIME ZONE,
    articles_found INTEGER DEFAULT 0
);

-- –ü—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤ –º–∏—Ä–∞
INSERT INTO language_coverage (language_code, language_name, native_name, region, countries) VALUES
('en', 'English', 'English', 'Global', ARRAY['US', 'UK', 'AU', 'CA']),
('es', 'Spanish', 'Espa√±ol', 'Global', ARRAY['ES', 'MX', 'AR', 'CO']),
('zh', 'Chinese', '‰∏≠Êñá', 'Asia', ARRAY['CN', 'TW', 'SG']),
('ar', 'Arabic', 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©', 'Middle East', ARRAY['SA', 'EG', 'AE']),
('hi', 'Hindi', '‡§π‡§ø‡§®‡•ç‡§¶‡•Ä', 'Asia', ARRAY['IN']),
('bn', 'Bengali', '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ', 'Asia', ARRAY['BD', 'IN']),
('pt', 'Portuguese', 'Portugu√™s', 'Global', ARRAY['BR', 'PT', 'AO']),
('ru', 'Russian', '–†—É—Å—Å–∫–∏–π', 'Europe/Asia', ARRAY['RU', 'BY', 'KZ']),
('ja', 'Japanese', 'Êó•Êú¨Ë™û', 'Asia', ARRAY['JP']),
('de', 'German', 'Deutsch', 'Europe', ARRAY['DE', 'AT', 'CH']),
('fr', 'French', 'Fran√ßais', 'Global', ARRAY['FR', 'CA', 'BE']),
('tr', 'Turkish', 'T√ºrk√ße', 'Asia/Europe', ARRAY['TR']),
('ko', 'Korean', 'ÌïúÍµ≠Ïñ¥', 'Asia', ARRAY['KR', 'KP']),
('it', 'Italian', 'Italiano', 'Europe', ARRAY['IT']),
('pl', 'Polish', 'Polski', 'Europe', ARRAY['PL']),
('uk', 'Ukrainian', '–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞', 'Europe', ARRAY['UA']),
('nl', 'Dutch', 'Nederlands', 'Europe', ARRAY['NL', 'BE']),
('sv', 'Swedish', 'Svenska', 'Europe', ARRAY['SE']),
('no', 'Norwegian', 'Norsk', 'Europe', ARRAY['NO']),
('da', 'Danish', 'Dansk', 'Europe', ARRAY['DK']),
('fi', 'Finnish', 'Suomi', 'Europe', ARRAY['FI']),
('is', 'Icelandic', '√çslenska', 'Europe', ARRAY['IS']),
('et', 'Estonian', 'Eesti', 'Europe', ARRAY['EE']),
('lv', 'Latvian', 'Latvie≈°u', 'Europe', ARRAY['LV']),
('lt', 'Lithuanian', 'Lietuvi≈≥', 'Europe', ARRAY['LT']),
('ro', 'Romanian', 'Rom√¢nƒÉ', 'Europe', ARRAY['RO']),
('bg', 'Bulgarian', '–ë—ä–ª–≥–∞—Ä—Å–∫–∏', 'Europe', ARRAY['BG']),
('hr', 'Croatian', 'Hrvatski', 'Europe', ARRAY['HR']),
('sr', 'Serbian', '–°—Ä–ø—Å–∫–∏', 'Europe', ARRAY['RS']),
('sk', 'Slovak', 'Slovenƒçina', 'Europe', ARRAY['SK']),
('sl', 'Slovenian', 'Sloven≈°ƒçina', 'Europe', ARRAY['SI']),
('cs', 'Czech', 'ƒåe≈°tina', 'Europe', ARRAY['CZ']),
('hu', 'Hungarian', 'Magyar', 'Europe', ARRAY['HU']),
('el', 'Greek', 'ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨', 'Europe', ARRAY['GR']),
('he', 'Hebrew', '◊¢◊ë◊®◊ô◊™', 'Middle East', ARRAY['IL']),
('fa', 'Persian', 'ŸÅÿßÿ±ÿ≥€å', 'Middle East', ARRAY['IR', 'AF']),
('ur', 'Urdu', 'ÿßÿ±ÿØŸà', 'Asia', ARRAY['PK', 'IN']),
('th', 'Thai', '‡πÑ‡∏ó‡∏¢', 'Asia', ARRAY['TH']),
('vi', 'Vietnamese', 'Ti·∫øng Vi·ªát', 'Asia', ARRAY['VN']),
('id', 'Indonesian', 'Bahasa Indonesia', 'Asia', ARRAY['ID']),
('ms', 'Malay', 'Bahasa Melayu', 'Asia', ARRAY['MY', 'SG']),
('tl', 'Filipino', 'Tagalog', 'Asia', ARRAY['PH']),
('sw', 'Swahili', 'Kiswahili', 'Africa', ARRAY['KE', 'TZ', 'UG']),
('am', 'Amharic', '·ä†·àõ·à≠·äõ', 'Africa', ARRAY['ET']),
('yo', 'Yoruba', 'Yor√πb√°', 'Africa', ARRAY['NG']),
('zu', 'Zulu', 'isiZulu', 'Africa', ARRAY['ZA']),
('xh', 'Xhosa', 'isiXhosa', 'Africa', ARRAY['ZA']),
('af', 'Afrikaans', 'Afrikaans', 'Africa', ARRAY['ZA']),
('ka', 'Georgian', '·É•·Éê·É†·Éó·É£·Éö·Éò', 'Asia', ARRAY['GE']),
('hy', 'Armenian', '’Ä’°’µ’•÷Ä’•’∂', 'Asia', ARRAY['AM']),
('az', 'Azerbaijani', 'Az…ôrbaycan', 'Asia', ARRAY['AZ']),
('kk', 'Kazakh', '“ö–∞–∑–∞“õ', 'Asia', ARRAY['KZ']),
('ky', 'Kyrgyz', '–ö—ã—Ä–≥—ã–∑—á–∞', 'Asia', ARRAY['KG']),
('uz', 'Uzbek', 'O ªzbek', 'Asia', ARRAY['UZ']),
('tg', 'Tajik', '–¢–æ“∑–∏–∫”£', 'Asia', ARRAY['TJ']),
('mn', 'Mongolian', '–ú–æ–Ω–≥–æ–ª', 'Asia', ARRAY['MN']),
('ne', 'Nepali', '‡§®‡•á‡§™‡§æ‡§≤‡•Ä', 'Asia', ARRAY['NP']),
('si', 'Sinhala', '‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω', 'Asia', ARRAY['LK']),
('my', 'Burmese', '·Äô·Äº·Äî·Ä∫·Äô·Ä¨', 'Asia', ARRAY['MM']),
('km', 'Khmer', '·ûÅ·üí·ûò·üÇ·ûö', 'Asia', ARRAY['KH']),
('lo', 'Lao', '‡∫•‡∫≤‡∫ß', 'Asia', ARRAY['LA']),
('qu', 'Quechua', 'Runa Simi', 'South America', ARRAY['PE', 'BO', 'EC']),
('gn', 'Guarani', 'Ava√±e·∫Ω', 'South America', ARRAY['PY']),
('ay', 'Aymara', 'Aymar aru', 'South America', ARRAY['BO', 'PE']),
('ht', 'Haitian Creole', 'Krey√≤l ayisyen', 'Caribbean', ARRAY['HT']);

-- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX idx_articles_sentiment ON press_articles(sentiment);
CREATE INDEX idx_articles_published ON press_articles(published_date DESC);
CREATE INDEX idx_articles_language ON press_articles(source_language);
CREATE INDEX idx_articles_country ON press_articles(source_country);
CREATE INDEX idx_articles_url ON press_articles(url);
CREATE INDEX idx_language_coverage_checked ON language_coverage(last_checked);
```

## üì¶ –§–ê–ó–ê 2: –†–ê–°–®–ò–†–ï–ù–ò–ï STATE –ò SCHEMAS (–î–µ–Ω—å 3-4)

### 2.1 –ù–æ–≤—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ `state.py`

```python
from typing import TypedDict, List, Dict, Optional, Literal
from datetime import datetime
from langchain_core.messages import AnyMessage

class ArticleInfo(TypedDict):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ"""
    url: str
    title: str
    source_name: str
    source_country: str
    source_language: str
    language_name: str
    published_date: Optional[datetime]
    original_content: str
    translated_content: Optional[str]
    summary: str
    sentiment: Literal["positive", "negative", "neutral"]
    sentiment_score: float
    sentiment_explanation: str
    key_phrases: List[str]
    mentions_context: List[Dict[str, str]]  # {"text": "...", "context": "economic/political/cultural"}

class LanguageSearchState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —è–∑—ã–∫–µ"""
    language_code: str
    language_name: str
    search_queries: List[str]
    articles_found: List[ArticleInfo]
    search_completed: bool

class OrchestratorState(TypedDict):
    """–ì–ª–∞–≤–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
    messages: List[AnyMessage]
    search_mode: Literal["all_languages", "specific_languages", "regions"]
    target_languages: Optional[List[str]]  # –µ—Å–ª–∏ specific_languages
    target_regions: Optional[List[str]]    # –µ—Å–ª–∏ regions
    active_searches: Dict[str, LanguageSearchState]  # language_code -> state
    all_articles: List[ArticleInfo]
    positive_articles: List[ArticleInfo]
    negative_articles: List[ArticleInfo]
    neutral_articles: List[ArticleInfo]
    digest_generated: bool
    positive_digest: Optional[str]
    negative_digest: Optional[str]
    notification_sent: bool

class PressMonitorOverallState(TypedDict):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã"""
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
    messages: List[AnyMessage]
    
    # –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–µ—Å—Å—ã
    monitor_mode: bool  # —Ä–µ–∂–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ vs –æ–±—ã—á–Ω—ã–π research
    orchestrator_state: Optional[OrchestratorState]
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    languages_to_monitor: List[str]  # –∫–∞–∫–∏–µ —è–∑—ã–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å
    sentiment_threshold: float  # –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤/–Ω–µ–≥–∞—Ç–∏–≤
    max_articles_per_language: int
    translation_enabled: bool
```

### 2.2 –ù–æ–≤—ã–µ Pydantic —Å—Ö–µ–º—ã –≤ `tools_and_schemas.py`

```python
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Literal
from datetime import datetime

class LanguageSearchQuery(BaseModel):
    """–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞"""
    language_code: str = Field(description="ISO –∫–æ–¥ —è–∑—ã–∫–∞")
    language_name: str = Field(description="–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞")
    queries: List[str] = Field(
        description="–°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ",
        min_items=1,
        max_items=5
    )

class MultiLanguageSearchPlan(BaseModel):
    """–ü–ª–∞–Ω –ø–æ–∏—Å–∫–∞ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É —è–∑—ã–∫–æ–≤"""
    search_queries_by_language: List[LanguageSearchQuery] = Field(
        description="–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞"
    )
    
class SentimentAnalysis(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    sentiment: Literal["positive", "negative", "neutral"] = Field(
        description="–û–±—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞—Ç—å–∏"
    )
    score: float = Field(
        description="–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç -1 (–Ω–µ–≥–∞—Ç–∏–≤) –¥–æ 1 (–ø–æ–∑–∏—Ç–∏–≤)",
        ge=-1.0,
        le=1.0
    )
    explanation: str = Field(
        description="–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞"
    )
    key_phrases: List[str] = Field(
        description="–ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã, –æ–ø—Ä–µ–¥–µ–ª–∏–≤—à–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"
    )

class ArticleAnalysis(BaseModel):
    """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏"""
    title: str
    summary: str = Field(description="–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏")
    sentiment_analysis: SentimentAnalysis
    azerbaijan_mentions: List[Dict[str, str]] = Field(
        description="–í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
    )
    topics: List[str] = Field(
        description="–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã —Å—Ç–∞—Ç—å–∏"
    )
    
class PressDigest(BaseModel):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    digest_type: Literal["positive", "negative"]
    title: str
    summary: str = Field(description="–û–±—â–µ–µ —Ä–µ–∑—é–º–µ")
    articles_by_region: Dict[str, List[Dict[str, str]]] = Field(
        description="–°—Ç–∞—Ç—å–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º"
    )
    key_themes: List[str] = Field(description="–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã")
    statistics: Dict[str, int] = Field(
        description="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –∫–æ–ª-–≤–æ —Å—Ç–∞—Ç–µ–π, —è–∑—ã–∫–æ–≤, —Å—Ç—Ä–∞–Ω"
    )
```

## ü§ñ –§–ê–ó–ê 3: –°–û–ó–î–ê–ù–ò–ï –ù–û–í–´–• –ê–ì–ï–ù–¢–û–í (–î–µ–Ω—å 5-7)

### 3.1 –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤
```
backend/src/agent/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ app.py (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π)
‚îú‚îÄ‚îÄ configuration.py (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
‚îú‚îÄ‚îÄ graph.py (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π)
‚îú‚îÄ‚îÄ press_monitor_graph.py (–ù–û–í–´–ô - –≥—Ä–∞—Ñ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–µ—Å—Å—ã)
‚îú‚îÄ‚îÄ orchestrator.py (–ù–û–í–´–ô - –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä)
‚îú‚îÄ‚îÄ language_agents.py (–ù–û–í–´–ô - —è–∑—ã–∫–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã)  
‚îú‚îÄ‚îÄ sentiment_analyzer.py (–ù–û–í–´–ô - –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏)
‚îú‚îÄ‚îÄ digest_generator.py (–ù–û–í–´–ô - –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤)
‚îú‚îÄ‚îÄ temporal_analytics.py (–ù–û–í–´–ô - –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤)
‚îú‚îÄ‚îÄ prompts.py (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π)
‚îú‚îÄ‚îÄ press_prompts.py (–ù–û–í–´–ô - –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø—Ä–µ—Å—Å—ã)
‚îú‚îÄ‚îÄ state.py (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
‚îú‚îÄ‚îÄ tools_and_schemas.py (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
‚îú‚îÄ‚îÄ utils.py (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π)
‚îî‚îÄ‚îÄ database.py (–ù–û–í–´–ô - —Ä–∞–±–æ—Ç–∞ —Å –ë–î)
```

### 3.2 –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (`orchestrator.py`)

```python
from typing import List, Dict, Any
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import Send

from .state import OrchestratorState, LanguageSearchState
from .language_agents import create_language_search_queries, search_news_in_language
from .sentiment_analyzer import analyze_articles_sentiment
from .digest_generator import generate_digest
from .database import save_articles_to_db, get_uncovered_languages

def orchestrator_node(state: OrchestratorState) -> Dict[str, Any]:
    """–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä - —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —è–∑—ã–∫–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
    if state["search_mode"] == "all_languages":
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —è–∑—ã–∫–∏ –∏–∑ –ë–î, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞–≤–Ω–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å
        languages_to_check = get_uncovered_languages(hours_threshold=24)
    elif state["search_mode"] == "specific_languages":
        languages_to_check = state["target_languages"]
    else:  # regions
        languages_to_check = get_languages_by_regions(state["target_regions"])
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    search_tasks = []
    for lang_code in languages_to_check:
        search_tasks.append(
            Send(
                "language_search",
                {
                    "language_code": lang_code,
                    "language_name": LANGUAGE_NAMES[lang_code],
                    "search_queries": [],  # –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –≤ language_search
                    "articles_found": [],
                    "search_completed": False
                }
            )
        )
    
    return {
        "active_searches": {lang: {} for lang in languages_to_check},
        "messages": [HumanMessage(content=f"–ó–∞–ø—É—Å–∫–∞—é –ø–æ–∏—Å–∫ –ø–æ {len(languages_to_check)} —è–∑—ã–∫–∞–º...")]
    }

def should_generate_digest(state: OrchestratorState) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –ø–æ–∏—Å–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã"""
    return all(
        search.get("search_completed", False) 
        for search in state["active_searches"].values()
    )

def aggregate_results(state: OrchestratorState) -> Dict[str, Any]:
    """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
    all_articles = []
    
    for lang_code, search_state in state["active_searches"].items():
        all_articles.extend(search_state.get("articles_found", []))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    save_articles_to_db(all_articles)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    positive = [a for a in all_articles if a["sentiment"] == "positive"]
    negative = [a for a in all_articles if a["sentiment"] == "negative"]
    neutral = [a for a in all_articles if a["sentiment"] == "neutral"]
    
    return {
        "all_articles": all_articles,
        "positive_articles": positive,
        "negative_articles": negative,
        "neutral_articles": neutral,
        "messages": [HumanMessage(
            content=f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(all_articles)} "
            f"(–ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {len(positive)}, –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(negative)})"
        )]
    }

def create_orchestrator_graph():
    """–°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
    graph = StateGraph(OrchestratorState)
    
    # –£–∑–ª—ã
    graph.add_node("orchestrator", orchestrator_node)
    graph.add_node("language_search", search_news_in_language)
    graph.add_node("aggregate_results", aggregate_results)
    graph.add_node("sentiment_analysis", analyze_articles_sentiment)
    graph.add_node("generate_positive_digest", generate_positive_digest)
    graph.add_node("generate_negative_digest", generate_negative_digest)
    # –£–±—Ä–∞–ª–∏ send_notifications –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    # –°–≤—è–∑–∏
    graph.set_entry_point("orchestrator")
    graph.add_edge("orchestrator", "language_search")
    graph.add_conditional_edges(
        "language_search",
        should_generate_digest,
        {
            True: "aggregate_results",
            False: "language_search"
        }
    )
    graph.add_edge("aggregate_results", "sentiment_analysis")
    graph.add_edge("sentiment_analysis", "generate_positive_digest")
    graph.add_edge("generate_positive_digest", "generate_negative_digest")
    graph.add_edge("generate_negative_digest", END)
    
    return graph.compile()
```

### 3.3 –Ø–∑—ã–∫–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã (`language_agents.py`)

```python
import asyncio
from typing import Dict, List, Any
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI
from google import genai

from .state import LanguageSearchState, ArticleInfo
from .press_prompts import MULTI_LANGUAGE_SEARCH_PROMPT, ARTICLE_EXTRACTION_PROMPT
from .utils import clean_url

# –°–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å–ª–æ–≤–∞ "–ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω" –Ω–∞ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏
AZERBAIJAN_TRANSLATIONS = {
    "en": ["Azerbaijan", "Azerbaijani", "Azeri", "Baku"],
    "ru": ["–ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω", "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω—Å–∫–∏–π", "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–µ—Ü", "–ë–∞–∫—É"],
    "tr": ["Azerbaycan", "Azeri", "Azerbaycanlƒ±", "Bak√º"],
    "ar": ["ÿ£ÿ∞ÿ±ÿ®Ÿäÿ¨ÿßŸÜ", "ÿ£ÿ∞ÿ±ÿ®Ÿäÿ¨ÿßŸÜŸä", "ÿ®ÿßŸÉŸà"],
    "zh": ["ÈòøÂ°ûÊãúÁñÜ", "ÈòøÂ°ûÊãúÁñÜ‰∫∫", "Â∑¥Â∫ì"],
    "ja": ["„Ç¢„Çº„É´„Éê„Ç§„Ç∏„É£„É≥", "„Ç¢„Çº„É´„Éê„Ç§„Ç∏„É£„É≥‰∫∫", "„Éê„ÇØ„Éº"],
    "ko": ["ÏïÑÏ†úÎ•¥Î∞îÏù¥Ïûî", "ÏïÑÏ†úÎ•¥Î∞îÏù¥ÏûîÏù∏", "Î∞îÏø†"],
    "de": ["Aserbaidschan", "aserbaidschanisch", "Aserbaidschaner", "Baku"],
    "fr": ["Azerba√Ødjan", "azerba√Ødjanais", "Bakou"],
    "es": ["Azerbaiy√°n", "azerbaiyano", "Bak√∫"],
    "pt": ["Azerbaij√£o", "azerbaijano", "Baku"],
    "it": ["Azerbaigian", "azerbaigiano", "Baku"],
    "hi": ["‡§Ö‡§ú‡§º‡§∞‡§¨‡•à‡§ú‡§æ‡§®", "‡§Ö‡§ú‡§º‡§∞‡§¨‡•à‡§ú‡§æ‡§®‡•Ä", "‡§¨‡§æ‡§ï‡•Ç"],
    "fa": ["ÿ¢ÿ∞ÿ±ÿ®ÿß€åÿ¨ÿßŸÜ", "ÿ¢ÿ∞ÿ±ÿ®ÿß€åÿ¨ÿßŸÜ€å", "ÿ®ÿß⁄©Ÿà"],
    "he": ["◊ê◊ñ◊®◊ë◊ô◊ô◊í'◊ü", "◊ê◊ñ◊®◊ë◊ô◊ô◊í'◊†◊ô", "◊ë◊ê◊ß◊ï"],
    "is": ["Aserba√≠dsjan", "aserba√≠dsjanskt", "Bak√∫"],
    "sw": ["Azabajani", "Mwazabajani", "Baku"],
    "am": ["·ä†·ãò·à≠·â£·ã≠·åÉ·äï", "·ä†·ãò·à≠·â£·ã≠·åÉ·äì·ãä", "·â£·ä©"],
    # –î–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏...
}

async def create_language_search_queries(
    language_code: str, 
    language_name: str,
    model: ChatGoogleGenerativeAI
) -> List[str]:
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞"""
    
    azerbaijan_terms = AZERBAIJAN_TRANSLATIONS.get(language_code, ["Azerbaijan"])
    
    prompt = MULTI_LANGUAGE_SEARCH_PROMPT.format(
        language_name=language_name,
        language_code=language_code,
        azerbaijan_terms=", ".join(azerbaijan_terms),
        current_date=datetime.now().strftime("%Y-%m-%d")
    )
    
    response = await model.ainvoke(prompt)
    
    # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
    queries = []
    for line in response.content.split("\n"):
        if line.strip() and not line.startswith("#"):
            queries.append(line.strip())
    
    return queries[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —è–∑—ã–∫

async def search_news_in_language(state: LanguageSearchState) -> Dict[str, Any]:
    """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —è–∑—ã–∫–µ"""
    
    client = genai.Client()
    model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –µ—Å–ª–∏ –∏—Ö –µ—â–µ –Ω–µ—Ç
    if not state["search_queries"]:
        queries = await create_language_search_queries(
            state["language_code"],
            state["language_name"],
            model
        )
        state["search_queries"] = queries
    
    articles_found = []
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    for query in state["search_queries"]:
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —è–∑—ã–∫–æ–≤–æ–π —Ñ–∏–ª—å—Ç—Ä –∫ –∑–∞–ø—Ä–æ—Å—É
            language_filtered_query = f"{query} language:{state['language_code']}"
            
            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=language_filtered_query,
                config={
                    "tools": [{"google_search": {}}],
                    "temperature": 0.7,
                }
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'grounding_metadata'):
                        for source in candidate.grounding_metadata.grounding_sources:
                            article = await extract_article_info(
                                source,
                                state["language_code"],
                                state["language_name"],
                                model
                            )
                            if article:
                                articles_found.append(article)
        
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞ {state['language_name']}: {e}")
            continue
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        await asyncio.sleep(1)
    
    return {
        "articles_found": articles_found,
        "search_completed": True,
        "active_searches": {
            state["language_code"]: {
                **state,
                "articles_found": articles_found,
                "search_completed": True
            }
        }
    }

async def extract_article_info(
    source: Any,
    language_code: str,
    language_name: str,
    model: ChatGoogleGenerativeAI
) -> Optional[ArticleInfo]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ"""
    
    try:
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        url = clean_url(source.uri)
        title = source.title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ —Å—Ç—Ä–∞–Ω—É
        source_info = extract_source_info(url)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
        content = source.text or ""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å—é
        prompt = ARTICLE_EXTRACTION_PROMPT.format(
            title=title,
            content=content[:3000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            language_name=language_name,
            azerbaijan_terms=", ".join(AZERBAIJAN_TRANSLATIONS.get(language_code, ["Azerbaijan"]))
        )
        
        analysis = await model.ainvoke(prompt)
        parsed_analysis = parse_article_analysis(analysis.content)
        
        return ArticleInfo(
            url=url,
            title=title,
            source_name=source_info["name"],
            source_country=source_info["country"],
            source_language=language_code,
            language_name=language_name,
            published_date=extract_date(source),
            original_content=content,
            translated_content=None,  # –ë—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            summary=parsed_analysis["summary"],
            sentiment="neutral",  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ sentiment_analyzer
            sentiment_score=0.0,
            sentiment_explanation="",
            key_phrases=parsed_analysis["key_phrases"],
            mentions_context=parsed_analysis["mentions"]
        )
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        return None
```

### 3.4 –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (`sentiment_analyzer.py`)

```python
from typing import Dict, List, Any
from langchain_google_genai import ChatGoogleGenerativeAI

from .state import OrchestratorState, ArticleInfo
from .press_prompts import SENTIMENT_ANALYSIS_PROMPT
from .tools_and_schemas import SentimentAnalysis

async def analyze_articles_sentiment(state: OrchestratorState) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    
    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro-preview-05-06",
        temperature=0.3
    )
    
    analyzed_articles = []
    
    for article in state["all_articles"]:
        # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Å–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∏–º
        content_to_analyze = article["original_content"]
        if article["source_language"] != "en" and state.get("translation_enabled", True):
            content_to_analyze = await translate_content(
                article["original_content"],
                article["source_language"],
                model
            )
            article["translated_content"] = content_to_analyze
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        prompt = SENTIMENT_ANALYSIS_PROMPT.format(
            title=article["title"],
            content=content_to_analyze[:3000],
            summary=article["summary"],
            mentions=article["mentions_context"]
        )
        
        response = await model.ainvoke(
            prompt,
            output_parser=SentimentAnalysis
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ
        article["sentiment"] = response.sentiment
        article["sentiment_score"] = response.score
        article["sentiment_explanation"] = response.explanation
        article["key_phrases"].extend(response.key_phrases)
        
        analyzed_articles.append(article)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    positive = [a for a in analyzed_articles if a["sentiment"] == "positive"]
    negative = [a for a in analyzed_articles if a["sentiment"] == "negative"]
    neutral = [a for a in analyzed_articles if a["sentiment"] == "neutral"]
    
    return {
        "all_articles": analyzed_articles,
        "positive_articles": positive,
        "negative_articles": negative,
        "neutral_articles": neutral,
        "messages": state["messages"] + [
            HumanMessage(content=f"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω. "
                               f"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {len(positive)}, "
                               f"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(negative)}, "
                               f"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {len(neutral)}")
        ]
    }

async def translate_content(
    content: str,
    source_language: str,
    model: ChatGoogleGenerativeAI
) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
    
    prompt = f"""Translate the following text from {source_language} to English.
    Preserve the meaning and tone as accurately as possible.
    
    Text:
    {content[:3000]}
    
    Translation:"""
    
    response = await model.ainvoke(prompt)
    return response.content
```

### 3.5 –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ (`digest_generator.py`)

```python
from typing import Dict, List, Any
from datetime import datetime
from collections import defaultdict
from langchain_google_genai import ChatGoogleGenerativeAI

from .state import OrchestratorState
from .press_prompts import DIGEST_GENERATION_PROMPT
from .database import save_digest_to_db

async def generate_positive_digest(state: OrchestratorState) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç"""
    
    if not state["positive_articles"]:
        return {
            "positive_digest": "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
            "digest_generated": True
        }
    
    digest = await generate_digest(
        articles=state["positive_articles"],
        digest_type="positive",
        model=ChatGoogleGenerativeAI(model="gemini-2.5-pro-preview-05-06")
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    save_digest_to_db(digest, "positive", state["positive_articles"])
    
    return {
        "positive_digest": digest,
        "messages": state["messages"] + [
            HumanMessage(content="–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        ]
    }

async def generate_negative_digest(state: OrchestratorState) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç"""
    
    if not state["negative_articles"]:
        return {
            "negative_digest": "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
            "digest_generated": True
        }
    
    digest = await generate_digest(
        articles=state["negative_articles"],
        digest_type="negative",
        model=ChatGoogleGenerativeAI(model="gemini-2.5-pro-preview-05-06")
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    save_digest_to_db(digest, "negative", state["negative_articles"])
    
    return {
        "negative_digest": digest,
        "digest_generated": True,
        "messages": state["messages"] + [
            HumanMessage(content="–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        ]
    }

async def generate_digest(
    articles: List[ArticleInfo],
    digest_type: str,
    model: ChatGoogleGenerativeAI
) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –∏–∑ —Å—Ç–∞—Ç–µ–π"""
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ —è–∑—ã–∫–∞–º
    articles_by_region = defaultdict(list)
    articles_by_language = defaultdict(int)
    
    for article in articles:
        region = get_region_by_language(article["source_language"])
        articles_by_region[region].append(article)
        articles_by_language[article["language_name"]] += 1
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    all_topics = []
    for article in articles:
        all_topics.extend(article.get("topics", []))
    
    key_themes = list(set(all_topics))[:10]  # –¢–æ–ø-10 —Ç–µ–º
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    articles_data = []
    for region, region_articles in articles_by_region.items():
        articles_data.append({
            "region": region,
            "articles": [
                {
                    "title": a["title"],
                    "source": f"{a['source_name']} ({a['source_country']})",
                    "language": a["language_name"],
                    "summary": a["summary"],
                    "sentiment_explanation": a["sentiment_explanation"],
                    "url": a["url"]
                }
                for a in region_articles[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Å—Ç–∞—Ç–µ–π –Ω–∞ —Ä–µ–≥–∏–æ–Ω
            ]
        })
    
    prompt = DIGEST_GENERATION_PROMPT.format(
        digest_type=digest_type,
        total_articles=len(articles),
        languages_count=len(articles_by_language),
        languages_list=", ".join(articles_by_language.keys()),
        regions_count=len(articles_by_region),
        articles_by_region=articles_data,
        key_themes=key_themes,
        date=datetime.now().strftime("%Y-%m-%d")
    )
    
    response = await model.ainvoke(prompt)
    return response.content

def get_region_by_language(language_code: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–≥–∏–æ–Ω –ø–æ –∫–æ–¥—É —è–∑—ã–∫–∞"""
    
    LANGUAGE_REGIONS = {
        # –ï–≤—Ä–æ–ø–∞
        "en": "Europe/Global",
        "de": "Europe",
        "fr": "Europe/Global", 
        "es": "Europe/Latin America",
        "it": "Europe",
        "pt": "Europe/Latin America",
        "ru": "Europe/Asia",
        "pl": "Europe",
        "uk": "Europe",
        "ro": "Europe",
        "nl": "Europe",
        "el": "Europe",
        "hu": "Europe",
        "cs": "Europe",
        "sv": "Europe",
        "da": "Europe",
        "fi": "Europe",
        "no": "Europe",
        "is": "Europe",
        "et": "Europe",
        "lv": "Europe",
        "lt": "Europe",
        "bg": "Europe",
        "hr": "Europe",
        "sr": "Europe",
        "sk": "Europe",
        "sl": "Europe",
        
        # –ê–∑–∏—è
        "zh": "Asia",
        "ja": "Asia",
        "ko": "Asia",
        "hi": "Asia",
        "bn": "Asia",
        "ur": "Asia",
        "fa": "Asia",
        "ar": "Middle East",
        "he": "Middle East",
        "tr": "Asia/Europe",
        "th": "Asia",
        "vi": "Asia",
        "id": "Asia",
        "ms": "Asia",
        "tl": "Asia",
        "my": "Asia",
        "km": "Asia",
        "lo": "Asia",
        "ne": "Asia",
        "si": "Asia",
        "ka": "Asia",
        "hy": "Asia",
        "az": "Asia",
        "kk": "Asia",
        "ky": "Asia",
        "uz": "Asia",
        "tg": "Asia",
        "mn": "Asia",
        
        # –ê—Ñ—Ä–∏–∫–∞
        "sw": "Africa",
        "am": "Africa",
        "yo": "Africa",
        "zu": "Africa",
        "xh": "Africa",
        "af": "Africa",
        
        # –ê–º–µ—Ä–∏–∫–∞
        "qu": "South America",
        "gn": "South America",
        "ay": "South America",
        "ht": "Caribbean",
    }
    
    return LANGUAGE_REGIONS.get(language_code, "Other")
```

### 3.6 –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç (`temporal_analytics.py`)

```python
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd
from langchain_google_genai import ChatGoogleGenerativeAI

from .state import TemporalAnalyticsState
from .database import get_historical_articles, get_sentiment_trends
from .press_prompts import TEMPORAL_ANALYSIS_PROMPT, TREND_COMPARISON_PROMPT

class TemporalAnalyticsAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –∏ —Ä–µ–≥–∏–æ–Ω–∞–º"""
    
    def __init__(self):
        self.model = ChatGoogleGenerativeAI(
            model="gemini-2.5-pro-preview-05-06",
            temperature=0.3
        )
    
    async def analyze_temporal_changes(
        self,
        country: Optional[str] = None,
        region: Optional[str] = None,
        time_period_days: int = 30,
        comparison_periods: List[int] = [7, 30, 90]
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        historical_data = await get_historical_articles(
            country=country,
            region=region,
            days_back=max(comparison_periods)
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥—ã –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
        trends = {}
        for period in comparison_periods:
            trends[f"{period}_days"] = await self._analyze_period(
                historical_data,
                period,
                country,
                region
            )
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        detailed_analysis = await self._detailed_temporal_analysis(
            historical_data,
            time_period_days
        )
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤
        predictions = await self._predict_trends(trends, detailed_analysis)
        
        return {
            "country": country,
            "region": region,
            "trends": trends,
            "detailed_analysis": detailed_analysis,
            "predictions": predictions,
            "generated_at": datetime.now()
        }
    
    async def _analyze_period(
        self,
        articles: List[Dict],
        period_days: int,
        country: Optional[str],
        region: Optional[str]
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
        
        cutoff_date = datetime.now() - timedelta(days=period_days)
        period_articles = [
            a for a in articles 
            if a["published_date"] >= cutoff_date
        ]
        
        # –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        total = len(period_articles)
        positive = len([a for a in period_articles if a["sentiment"] == "positive"])
        negative = len([a for a in period_articles if a["sentiment"] == "negative"])
        neutral = len([a for a in period_articles if a["sentiment"] == "neutral"])
        
        # –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
        positive_pct = (positive / total * 100) if total > 0 else 0
        negative_pct = (negative / total * 100) if total > 0 else 0
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞–º
        topics_count = defaultdict(int)
        for article in period_articles:
            for topic in article.get("topics", []):
                topics_count[topic] += 1
        
        # –¢–æ–ø —Ç–µ–º—ã
        top_topics = sorted(
            topics_count.items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]
        
        return {
            "period_days": period_days,
            "total_articles": total,
            "sentiment_breakdown": {
                "positive": positive,
                "negative": negative,
                "neutral": neutral,
                "positive_percentage": positive_pct,
                "negative_percentage": negative_pct
            },
            "top_topics": top_topics,
            "daily_average": total / period_days if period_days > 0 else 0
        }
    
    async def _detailed_temporal_analysis(
        self,
        articles: List[Dict],
        period_days: int
    ) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –ø–æ –¥–Ω—è–º/–Ω–µ–¥–µ–ª—è–º"""
        
        cutoff_date = datetime.now() - timedelta(days=period_days)
        period_articles = [
            a for a in articles 
            if a["published_date"] >= cutoff_date
        ]
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º
        daily_sentiment = defaultdict(lambda: {"positive": 0, "negative": 0, "neutral": 0})
        
        for article in period_articles:
            day_key = article["published_date"].strftime("%Y-%m-%d")
            sentiment = article["sentiment"]
            daily_sentiment[day_key][sentiment] += 1
        
        # –ü–æ–∏—Å–∫ —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        trend_changes = await self._identify_trend_changes(daily_sentiment)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –∏–∑–º–µ–Ω–µ–Ω–∏–π
        change_reasons = await self._analyze_change_reasons(
            period_articles,
            trend_changes
        )
        
        return {
            "daily_sentiment": dict(daily_sentiment),
            "trend_changes": trend_changes,
            "change_reasons": change_reasons,
            "volatility_score": self._calculate_volatility(daily_sentiment)
        }
    
    async def _identify_trend_changes(
        self,
        daily_sentiment: Dict[str, Dict[str, int]]
    ) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤"""
        
        changes = []
        dates = sorted(daily_sentiment.keys())
        
        for i in range(1, len(dates)):
            prev_date = dates[i-1]
            curr_date = dates[i]
            
            prev_sentiment = daily_sentiment[prev_date]
            curr_sentiment = daily_sentiment[curr_date]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            prev_positive_ratio = prev_sentiment["positive"] / max(sum(prev_sentiment.values()), 1)
            curr_positive_ratio = curr_sentiment["positive"] / max(sum(curr_sentiment.values()), 1)
            
            change_pct = (curr_positive_ratio - prev_positive_ratio) * 100
            
            # –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–µ 20%, —Ñ–∏–∫—Å–∏—Ä—É–µ–º
            if abs(change_pct) > 20:
                changes.append({
                    "date": curr_date,
                    "change_percentage": change_pct,
                    "direction": "improvement" if change_pct > 0 else "deterioration",
                    "from_sentiment": prev_sentiment,
                    "to_sentiment": curr_sentiment
                })
        
        return changes
    
    async def _analyze_change_reasons(
        self,
        articles: List[Dict],
        trend_changes: List[Dict]
    ) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏—á–∏–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤"""
        
        reasons = []
        
        for change in trend_changes:
            change_date = datetime.strptime(change["date"], "%Y-%m-%d")
            
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ç—å–∏ –æ–∫–æ–ª–æ –¥–∞—Ç—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è (¬±2 –¥–Ω—è)
            relevant_articles = [
                a for a in articles
                if abs((a["published_date"] - change_date).days) <= 2
            ]
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ LLM
            if relevant_articles:
                articles_summary = "\n".join([
                    f"- {a['title']} ({a['sentiment']}): {a['summary']}"
                    for a in relevant_articles[:10]
                ])
                
                prompt = TREND_COMPARISON_PROMPT.format(
                    date=change["date"],
                    change_direction=change["direction"],
                    change_percentage=change["change_percentage"],
                    articles_summary=articles_summary
                )
                
                analysis = await self.model.ainvoke(prompt)
                
                reasons.append({
                    "date": change["date"],
                    "change": change,
                    "likely_reasons": analysis.content,
                    "supporting_articles": len(relevant_articles)
                })
        
        return reasons
    
    def _calculate_volatility(self, daily_sentiment: Dict[str, Dict[str, int]]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        
        if len(daily_sentiment) < 2:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ positive ratios
        daily_ratios = []
        for sentiment in daily_sentiment.values():
            total = sum(sentiment.values())
            if total > 0:
                daily_ratios.append(sentiment["positive"] / total)
        
        if not daily_ratios:
            return 0.0
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–∞–∫ –º–µ—Ä–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        mean_ratio = sum(daily_ratios) / len(daily_ratios)
        variance = sum((r - mean_ratio) ** 2 for r in daily_ratios) / len(daily_ratios)
        
        return variance ** 0.5
    
    async def _predict_trends(
        self,
        historical_trends: Dict[str, Any],
        detailed_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –±—É–¥—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        trend_summary = {
            "7_day_trend": historical_trends["7_days"]["sentiment_breakdown"],
            "30_day_trend": historical_trends["30_days"]["sentiment_breakdown"],
            "90_day_trend": historical_trends["90_days"]["sentiment_breakdown"],
            "volatility": detailed_analysis["volatility_score"],
            "recent_changes": detailed_analysis["trend_changes"][-3:] if detailed_analysis["trend_changes"] else []
        }
        
        prompt = f"""Based on the following sentiment trends for Azerbaijan coverage:

7-day trend: {trend_summary['7_day_trend']}
30-day trend: {trend_summary['30_day_trend']}
90-day trend: {trend_summary['90_day_trend']}
Volatility score: {trend_summary['volatility']}
Recent changes: {trend_summary['recent_changes']}

Predict:
1. Likely sentiment trend for next 7 days
2. Likely sentiment trend for next 30 days
3. Key factors that could influence the trend
4. Confidence level (low/medium/high)

Format as JSON."""

        prediction = await self.model.ainvoke(prompt)
        
        return {
            "next_7_days": "improving/stable/declining",
            "next_30_days": "improving/stable/declining", 
            "influencing_factors": ["factor1", "factor2"],
            "confidence": "medium",
            "raw_prediction": prediction.content
        }

async def create_temporal_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """–£–∑–µ–ª –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ –≥—Ä–∞—Ñ–µ"""
    
    agent = TemporalAnalyticsAgent()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∞–º —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö
    countries_to_analyze = await get_countries_with_sufficient_data(
        min_articles=50,
        days_back=90
    )
    
    temporal_analyses = {}
    
    for country in countries_to_analyze[:10]:  # –¢–æ–ø 10 —Å—Ç—Ä–∞–Ω
        analysis = await agent.analyze_temporal_changes(
            country=country,
            time_period_days=30,
            comparison_periods=[7, 30, 90]
        )
        temporal_analyses[country] = analysis
    
    # –¢–∞–∫–∂–µ –¥–µ–ª–∞–µ–º —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    regional_analyses = {}
    regions = ["Asia", "Europe", "Middle East", "Americas", "Africa"]
    
    for region in regions:
        analysis = await agent.analyze_temporal_changes(
            region=region,
            time_period_days=30,
            comparison_periods=[7, 30, 90]
        )
        regional_analyses[region] = analysis
    
    return {
        "temporal_analyses": {
            "by_country": temporal_analyses,
            "by_region": regional_analyses,
            "generated_at": datetime.now()
        },
        "messages": state["messages"] + [
            HumanMessage(content=f"Temporal analysis completed for {len(temporal_analyses)} countries and {len(regional_analyses)} regions")
        ]
    }

# –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥—Ä–∞—Ñ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
def create_enhanced_orchestrator_graph():
    """–°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    graph = StateGraph(OrchestratorState)
    
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —É–∑–ª—ã...
    graph.add_node("orchestrator", orchestrator_node)
    graph.add_node("language_search", search_news_in_language)
    graph.add_node("aggregate_results", aggregate_results)
    graph.add_node("sentiment_analysis", analyze_articles_sentiment)
    graph.add_node("temporal_analysis", create_temporal_analysis_node)  # –ù–û–í–´–ô
    graph.add_node("generate_positive_digest", generate_positive_digest)
    graph.add_node("generate_negative_digest", generate_negative_digest)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
    graph.add_edge("sentiment_analysis", "temporal_analysis")
    graph.add_edge("temporal_analysis", "generate_positive_digest")
    
    return graph.compile()
```

### 3.7 –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —Å—Ö–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ `state.py`

```python
class TemporalAnalyticsState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    country: Optional[str]
    region: Optional[str]
    time_periods: List[int]  # [7, 30, 90] –¥–Ω–µ–π
    sentiment_trends: Dict[str, Dict[str, float]]  # –ø–µ—Ä–∏–æ–¥ -> —Ç—Ä–µ–Ω–¥—ã
    volatility_scores: Dict[str, float]
    trend_predictions: Dict[str, str]
    significant_events: List[Dict[str, Any]]
    comparison_data: Dict[str, Any]
```

## üîÑ –§–ê–ó–ê 4: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° FRONTEND (–î–µ–Ω—å 8-9)

### 4.1 –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `app.py`

```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import RedirectResponse
from fastapi.staticfiles import StaticFiles
from langgraph.graph import StateGraph
from typing import Literal

from .graph import create_graph  # —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ
from .press_monitor_graph import create_press_monitor_graph  # –Ω–æ–≤—ã–π –≥—Ä–∞—Ñ

# –°–æ–∑–¥–∞–µ–º –æ–±–∞ –≥—Ä–∞—Ñ–∞
research_graph = create_graph()
press_monitor_graph = create_press_monitor_graph()

# –û–±–Ω–æ–≤–ª—è–µ–º langgraph.json –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –æ–±–æ–∏—Ö –≥—Ä–∞—Ñ–æ–≤
LANGGRAPH_CONFIG = {
    "graphs": {
        "research_agent": {
            "graph": research_graph,
            "description": "Original research agent"
        },
        "press_monitor": {
            "graph": press_monitor_graph,
            "description": "Azerbaijan press monitoring agent"
        }
    }
}

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.post("/api/press-monitor/start")
async def start_press_monitoring(
    mode: Literal["all_languages", "specific_languages", "regions"],
    languages: Optional[List[str]] = None,
    regions: Optional[List[str]] = None
):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–µ—Å—Å—ã"""
    
    config = {
        "configurable": {
            "search_mode": mode,
            "target_languages": languages,
            "target_regions": regions,
            "translation_enabled": True,
            "max_articles_per_language": 10
        }
    }
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ
    result = await press_monitor_graph.ainvoke(
        {"messages": [HumanMessage(content="Start press monitoring")]},
        config=config
    )
    
    return {
        "status": "started",
        "mode": mode,
        "result": result
    }

@app.get("/api/press-monitor/latest-digest")
async def get_latest_digest(digest_type: Literal["positive", "negative", "both"]):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç"""
    
    if digest_type == "both":
        positive = get_latest_digest_from_db("positive")
        negative = get_latest_digest_from_db("negative")
        return {
            "positive": positive,
            "negative": negative
        }
    else:
        digest = get_latest_digest_from_db(digest_type)
        return {"digest": digest}

@app.get("/api/press-monitor/statistics")
async def get_statistics():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    
    stats = get_monitoring_statistics()
    return stats
```

### 4.2 –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ React –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```typescript
// frontend/src/components/PressMonitor.tsx
import React, { useState } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Button } from "@/components/ui/button";
import { Select } from "@/components/ui/select";
import { Badge } from "@/components/ui/badge";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";

interface PressMonitorProps {
  onStartMonitoring: (mode: string, options?: any) => void;
}

export const PressMonitor: React.FC<PressMonitorProps> = ({ onStartMonitoring }) => {
  const [mode, setMode] = useState<'all_languages' | 'specific_languages' | 'regions'>('all_languages');
  const [selectedLanguages, setSelectedLanguages] = useState<string[]>([]);
  const [selectedRegions, setSelectedRegions] = useState<string[]>([]);
  
  const languages = [
    { code: 'en', name: 'English' },
    { code: 'ru', name: 'Russian' },
    { code: 'tr', name: 'Turkish' },
    { code: 'ar', name: 'Arabic' },
    { code: 'zh', name: 'Chinese' },
    { code: 'ja', name: 'Japanese' },
    { code: 'ko', name: 'Korean' },
    { code: 'de', name: 'German' },
    { code: 'fr', name: 'French' },
    { code: 'es', name: 'Spanish' },
    { code: 'pt', name: 'Portuguese' },
    { code: 'it', name: 'Italian' },
    { code: 'hi', name: 'Hindi' },
    { code: 'fa', name: 'Persian' },
    { code: 'he', name: 'Hebrew' },
    { code: 'is', name: 'Icelandic' },
    { code: 'sw', name: 'Swahili' },
    { code: 'am', name: 'Amharic' },
    // ... –¥–æ–±–∞–≤–∏—Ç—å –≤—Å–µ —è–∑—ã–∫–∏
  ];
  
  const regions = [
    'Europe',
    'Asia', 
    'Middle East',
    'Africa',
    'North America',
    'South America',
    'Oceania'
  ];
  
  const handleStart = () => {
    const options: any = {};
    
    if (mode === 'specific_languages') {
      options.languages = selectedLanguages;
    } else if (mode === 'regions') {
      options.regions = selectedRegions;
    }
    
    onStartMonitoring(mode, options);
  };
  
  return (
    <Card className="w-full">
      <CardHeader>
        <CardTitle>üåç Azerbaijan Press Monitor</CardTitle>
      </CardHeader>
      <CardContent>
        <div className="space-y-4">
          <div>
            <label className="block text-sm font-medium mb-2">
              Monitoring Mode
            </label>
            <Select value={mode} onValueChange={(value: any) => setMode(value)}>
              <option value="all_languages">All Languages (Global Coverage)</option>
              <option value="specific_languages">Specific Languages</option>
              <option value="regions">By Regions</option>
            </Select>
          </div>
          
          {mode === 'specific_languages' && (
            <div>
              <label className="block text-sm font-medium mb-2">
                Select Languages
              </label>
              <div className="grid grid-cols-3 gap-2 max-h-60 overflow-y-auto">
                {languages.map(lang => (
                  <label key={lang.code} className="flex items-center space-x-2">
                    <input
                      type="checkbox"
                      value={lang.code}
                      checked={selectedLanguages.includes(lang.code)}
                      onChange={(e) => {
                        if (e.target.checked) {
                          setSelectedLanguages([...selectedLanguages, lang.code]);
                        } else {
                          setSelectedLanguages(selectedLanguages.filter(l => l !== lang.code));
                        }
                      }}
                    />
                    <span className="text-sm">{lang.name}</span>
                  </label>
                ))}
              </div>
            </div>
          )}
          
          {mode === 'regions' && (
            <div>
              <label className="block text-sm font-medium mb-2">
                Select Regions
              </label>
              <div className="grid grid-cols-2 gap-2">
                {regions.map(region => (
                  <label key={region} className="flex items-center space-x-2">
                    <input
                      type="checkbox"
                      value={region}
                      checked={selectedRegions.includes(region)}
                      onChange={(e) => {
                        if (e.target.checked) {
                          setSelectedRegions([...selectedRegions, region]);
                        } else {
                          setSelectedRegions(selectedRegions.filter(r => r !== region));
                        }
                      }}
                    />
                    <span className="text-sm">{region}</span>
                  </label>
                ))}
              </div>
            </div>
          )}
          
          <Button 
            onClick={handleStart}
            className="w-full"
            disabled={
              (mode === 'specific_languages' && selectedLanguages.length === 0) ||
              (mode === 'regions' && selectedRegions.length === 0)
            }
          >
            üöÄ Start Monitoring
          </Button>
        </div>
      </CardContent>
    </Card>
  );
};

// frontend/src/components/DigestView.tsx
import React from 'react';
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { Badge } from "@/components/ui/badge";
import ReactMarkdown from 'react-markdown';

interface DigestViewProps {
  positiveDigest: string | null;
  negativeDigest: string | null;
  statistics: any;
}

export const DigestView: React.FC<DigestViewProps> = ({ 
  positiveDigest, 
  negativeDigest, 
  statistics 
}) => {
  return (
    <div className="w-full space-y-4">
      <Card>
        <CardHeader>
          <CardTitle>üìä Monitoring Statistics</CardTitle>
        </CardHeader>
        <CardContent>
          <div className="grid grid-cols-4 gap-4">
            <div className="text-center">
              <div className="text-2xl font-bold">{statistics.totalArticles}</div>
              <div className="text-sm text-gray-600">Total Articles</div>
            </div>
            <div className="text-center">
              <div className="text-2xl font-bold text-green-600">{statistics.positiveCount}</div>
              <div className="text-sm text-gray-600">Positive</div>
            </div>
            <div className="text-center">
              <div className="text-2xl font-bold text-red-600">{statistics.negativeCount}</div>
              <div className="text-sm text-gray-600">Negative</div>
            </div>
            <div className="text-center">
              <div className="text-2xl font-bold">{statistics.languagesCount}</div>
              <div className="text-sm text-gray-600">Languages</div>
            </div>
          </div>
        </CardContent>
      </Card>
      
      <Tabs defaultValue="positive" className="w-full">
        <TabsList className="grid w-full grid-cols-2">
          <TabsTrigger value="positive">
            ‚úÖ Positive Digest ({statistics.positiveCount})
          </TabsTrigger>
          <TabsTrigger value="negative">
            ‚ùå Negative Digest ({statistics.negativeCount})
          </TabsTrigger>
        </TabsList>
        
        <TabsContent value="positive">
          <Card>
            <CardHeader>
              <CardTitle className="text-green-600">Positive Press Coverage</CardTitle>
            </CardHeader>
            <CardContent>
              {positiveDigest ? (
                <div className="prose max-w-none">
                  <ReactMarkdown>{positiveDigest}</ReactMarkdown>
                </div>
              ) : (
                <p className="text-gray-500">No positive articles found.</p>
              )}
            </CardContent>
          </Card>
        </TabsContent>
        
        <TabsContent value="negative">
          <Card>
            <CardHeader>
              <CardTitle className="text-red-600">Negative Press Coverage</CardTitle>
            </CardHeader>
            <CardContent>
              {negativeDigest ? (
                <div className="prose max-w-none">
                  <ReactMarkdown>{negativeDigest}</ReactMarkdown>
                </div>
              ) : (
                <p className="text-gray-500">No negative articles found.</p>
              )}
            </CardContent>
          </Card>
        </TabsContent>
      </Tabs>
    </div>
  );
};

// frontend/src/components/TemporalAnalytics.tsx
import React from 'react';
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Line } from 'recharts';
import { LineChart, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';

interface TemporalAnalyticsProps {
  temporalData: any;
  selectedCountry?: string;
  selectedRegion?: string;
}

export const TemporalAnalytics: React.FC<TemporalAnalyticsProps> = ({ 
  temporalData, 
  selectedCountry,
  selectedRegion 
}) => {
  const formatChartData = (data: any) => {
    // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    const dailySentiment = data.detailed_analysis?.daily_sentiment || {};
    return Object.entries(dailySentiment).map(([date, sentiment]: [string, any]) => ({
      date,
      positive: sentiment.positive,
      negative: sentiment.negative,
      neutral: sentiment.neutral,
      positiveRatio: sentiment.positive / (sentiment.positive + sentiment.negative + sentiment.neutral) * 100
    }));
  };

  const renderTrendIndicator = (trend: string) => {
    const icons = {
      improving: 'üìà',
      stable: '‚û°Ô∏è',
      declining: 'üìâ'
    };
    const colors = {
      improving: 'text-green-600',
      stable: 'text-yellow-600',
      declining: 'text-red-600'
    };
    return (
      <span className={colors[trend] || 'text-gray-600'}>
        {icons[trend] || '‚ùì'} {trend}
      </span>
    );
  };

  return (
    <div className="space-y-6">
      {/* –û–±–∑–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ */}
      <Card>
        <CardHeader>
          <CardTitle>üìä Temporal Trend Analysis</CardTitle>
        </CardHeader>
        <CardContent>
          <div className="grid grid-cols-3 gap-4">
            <div className="text-center">
              <h4 className="font-semibold mb-2">7-Day Trend</h4>
              {renderTrendIndicator(temporalData.trends?.['7_days']?.trend || 'stable')}
              <div className="text-sm text-gray-600 mt-1">
                {temporalData.trends?.['7_days']?.sentiment_breakdown?.positive || 0} positive /
                {temporalData.trends?.['7_days']?.sentiment_breakdown?.negative || 0} negative
              </div>
            </div>
            <div className="text-center">
              <h4 className="font-semibold mb-2">30-Day Trend</h4>
              {renderTrendIndicator(temporalData.trends?.['30_days']?.trend || 'stable')}
              <div className="text-sm text-gray-600 mt-1">
                {temporalData.trends?.['30_days']?.sentiment_breakdown?.positive || 0} positive /
                {temporalData.trends?.['30_days']?.sentiment_breakdown?.negative || 0} negative
              </div>
            </div>
            <div className="text-center">
              <h4 className="font-semibold mb-2">90-Day Trend</h4>
              {renderTrendIndicator(temporalData.trends?.['90_days']?.trend || 'stable')}
              <div className="text-sm text-gray-600 mt-1">
                {temporalData.trends?.['90_days']?.sentiment_breakdown?.positive || 0} positive /
                {temporalData.trends?.['90_days']?.sentiment_breakdown?.negative || 0} negative
              </div>
            </div>
          </div>
        </CardContent>
      </Card>

      {/* –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ */}
      <Card>
        <CardHeader>
          <CardTitle>Sentiment Over Time</CardTitle>
        </CardHeader>
        <CardContent>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={formatChartData(temporalData)}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="date" />
              <YAxis />
              <Tooltip />
              <Legend />
              <Line type="monotone" dataKey="positive" stroke="#22c55e" name="Positive" />
              <Line type="monotone" dataKey="negative" stroke="#ef4444" name="Negative" />
              <Line type="monotone" dataKey="neutral" stroke="#6b7280" name="Neutral" />
              <Line type="monotone" dataKey="positiveRatio" stroke="#3b82f6" name="Positive %" strokeDasharray="5 5" />
            </LineChart>
          </ResponsiveContainer>
        </CardContent>
      </Card>

      {/* –ü—Ä–æ–≥–Ω–æ–∑—ã */}
      <Card>
        <CardHeader>
          <CardTitle>üîÆ Future Predictions</CardTitle>
        </CardHeader>
        <CardContent>
          <div className="space-y-4">
            <div>
              <h4 className="font-semibold">Next 7 Days</h4>
              <p className="text-sm">{temporalData.predictions?.next_7_days || 'No prediction available'}</p>
            </div>
            <div>
              <h4 className="font-semibold">Next 30 Days</h4>
              <p className="text-sm">{temporalData.predictions?.next_30_days || 'No prediction available'}</p>
            </div>
            <div>
              <h4 className="font-semibold">Key Influencing Factors</h4>
              <ul className="list-disc list-inside text-sm">
                {(temporalData.predictions?.influencing_factors || []).map((factor: string, idx: number) => (
                  <li key={idx}>{factor}</li>
                ))}
              </ul>
            </div>
          </div>
        </CardContent>
      </Card>

      {/* –ó–Ω–∞—á–∏–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è */}
      {temporalData.detailed_analysis?.trend_changes?.length > 0 && (
        <Card>
          <CardHeader>
            <CardTitle>üîÑ Significant Trend Changes</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="space-y-2">
              {temporalData.detailed_analysis.trend_changes.map((change: any, idx: number) => (
                <div key={idx} className="border-l-4 border-blue-500 pl-4 py-2">
                  <div className="font-semibold">{change.date}</div>
                  <div className={change.direction === 'improvement' ? 'text-green-600' : 'text-red-600'}>
                    {change.direction} by {Math.abs(change.change_percentage).toFixed(1)}%
                  </div>
                  {temporalData.detailed_analysis.change_reasons?.[idx]?.likely_reasons && (
                    <div className="text-sm text-gray-600 mt-1">
                      {temporalData.detailed_analysis.change_reasons[idx].likely_reasons}
                    </div>
                  )}
                </div>
              ))}
            </div>
          </CardContent>
        </Card>
      )}
    </div>
  );
};
```

## üîê –§–ê–ó–ê 5: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (–î–µ–Ω—å 10-12)

### 5.1 –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤

```python
# backend/tests/test_press_monitor.py
import pytest
from unittest.mock import Mock, patch
from agent.orchestrator import orchestrator_node, create_orchestrator_graph
from agent.language_agents import create_language_search_queries, AZERBAIJAN_TRANSLATIONS

@pytest.mark.asyncio
async def test_language_search_queries():
    """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤"""
    
    mock_model = Mock()
    mock_model.ainvoke.return_value = Mock(
        content="–ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω –Ω–æ–≤–æ—Å—Ç–∏\n–ë–∞–∫—É —Å–æ–±—ã—Ç–∏—è\n–ö–∞—Ä–∞–±–∞—Ö —Å–∏—Ç—É–∞—Ü–∏—è"
    )
    
    queries = await create_language_search_queries("ru", "Russian", mock_model)
    
    assert len(queries) > 0
    assert any("–ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω" in q for q in queries)

@pytest.mark.asyncio
async def test_orchestrator_all_languages():
    """–¢–µ—Å—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –≤ —Ä–µ–∂–∏–º–µ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤"""
    
    state = {
        "search_mode": "all_languages",
        "messages": [],
        "active_searches": {}
    }
    
    with patch('agent.database.get_uncovered_languages') as mock_languages:
        mock_languages.return_value = ["en", "ru", "tr"]
        
        result = orchestrator_node(state)
        
        assert len(result["active_searches"]) == 3
        assert "en" in result["active_searches"]

def test_azerbaijan_translations_coverage():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤"""
    
    required_languages = ["en", "ru", "tr", "ar", "zh", "ja", "ko", "de", "fr", "es", 
                         "pt", "it", "hi", "fa", "he", "is", "sw", "am"]
    
    for lang in required_languages:
        assert lang in AZERBAIJAN_TRANSLATIONS
        assert len(AZERBAIJAN_TRANSLATIONS[lang]) >= 2  # –º–∏–Ω–∏–º—É–º 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞

@pytest.mark.asyncio
async def test_sentiment_analysis():
    """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    
    from agent.sentiment_analyzer import analyze_articles_sentiment
    
    state = {
        "all_articles": [
            {
                "title": "Azerbaijan wins international award",
                "original_content": "Azerbaijan has been recognized...",
                "source_language": "en",
                "summary": "Positive news about Azerbaijan",
                "mentions_context": [{"text": "Azerbaijan wins", "context": "achievement"}]
            }
        ],
        "messages": []
    }
    
    with patch('agent.sentiment_analyzer.ChatGoogleGenerativeAI') as mock_model:
        mock_response = Mock()
        mock_response.sentiment = "positive"
        mock_response.score = 0.8
        mock_response.explanation = "Positive achievement"
        mock_response.key_phrases = ["wins", "recognized"]
        
        mock_model.return_value.ainvoke.return_value = mock_response
        
        result = await analyze_articles_sentiment(state)
        
        assert len(result["positive_articles"]) == 1
        assert result["positive_articles"][0]["sentiment"] == "positive"
```

### 5.2 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
# backend/src/agent/performance_optimizer.py
import asyncio
from typing import List, Dict, Any
from functools import lru_cache
import aioredis
from datetime import datetime, timedelta

class PerformanceOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–µ—Å—Å—ã"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self._redis = None
    
    async def get_redis(self):
        if not self._redis:
            self._redis = await aioredis.from_url(self.redis_url)
        return self._redis
    
    @lru_cache(maxsize=1000)
    def get_cached_translation(self, text: str, source_lang: str, target_lang: str) -> str:
        """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç–∏"""
        cache_key = f"translation:{source_lang}:{target_lang}:{hash(text)}"
        return cache_key
    
    async def batch_process_languages(
        self, 
        languages: List[str], 
        process_func: callable,
        batch_size: int = 5
    ) -> List[Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–æ–≤ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        results = []
        for i in range(0, len(languages), batch_size):
            batch = languages[i:i + batch_size]
            batch_tasks = [process_func(lang) for lang in batch]
            batch_results = await asyncio.gather(*batch_tasks)
            results.extend(batch_results)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + batch_size < len(languages):
                await asyncio.sleep(2)
        
        return results
    
    async def cache_search_results(self, language: str, results: List[Dict]) -> None:
        """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        
        redis = await self.get_redis()
        cache_key = f"search_results:{language}:{datetime.now().strftime('%Y-%m-%d')}"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ 24 —á–∞—Å–∞
        await redis.setex(
            cache_key,
            86400,  # 24 —á–∞—Å–∞
            json.dumps(results)
        )
    
    async def get_cached_search_results(self, language: str) -> Optional[List[Dict]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        redis = await self.get_redis()
        cache_key = f"search_results:{language}:{datetime.now().strftime('%Y-%m-%d')}"
        
        cached = await redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None
    
    def should_skip_language(self, language: str, last_checked: datetime) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —è–∑—ã–∫"""
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–∑—ã–∫–∏, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–µ–Ω–µ–µ 6 —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥
        if datetime.now() - last_checked < timedelta(hours=6):
            return True
        
        # –ú–µ–Ω–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–µ
        rare_languages = ['ay', 'gn', 'qu', 'ht']
        if language in rare_languages and datetime.now() - last_checked < timedelta(days=3):
            return True
        
        return False

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ orchestrator.py
optimizer = PerformanceOptimizer()

async def optimized_orchestrator_node(state: OrchestratorState) -> Dict[str, Any]:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä"""
    
    # –ü–æ–ª—É—á–∞–µ–º —è–∑—ã–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    all_languages = get_all_languages_from_db()
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —è–∑—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–∞–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å
    languages_to_check = []
    for lang in all_languages:
        last_checked = get_language_last_checked(lang)
        if not optimizer.should_skip_language(lang, last_checked):
            languages_to_check.append(lang)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —è–∑—ã–∫–∏ –±–∞—Ç—á–∞–º–∏
    search_tasks = await optimizer.batch_process_languages(
        languages_to_check,
        lambda lang: create_language_search_task(lang),
        batch_size=5
    )
    
    return {
        "active_searches": {lang: {} for lang in languages_to_check},
        "messages": [HumanMessage(
            content=f"–ó–∞–ø—É—Å–∫–∞—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ {len(languages_to_check)} —è–∑—ã–∫–∞–º..."
        )]
    }
```

## üöÄ –§–ê–ó–ê 6: –î–ï–ü–õ–û–ô –ò –ú–û–ù–ò–¢–û–†–ò–ù–ì (–î–µ–Ω—å 13-14)

### 6.1 –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Docker –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

```dockerfile
# Dockerfile (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π)
FROM python:3.11-slim as backend-builder

WORKDIR /app/backend

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è PostgreSQL
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

COPY backend/pyproject.toml backend/README.md ./
COPY backend/src ./src

RUN pip install --no-cache-dir .

# Frontend builder –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
FROM node:20-alpine as frontend-builder
# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

# Final stage —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º cron –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
FROM python:3.11-slim

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cron
RUN apt-get update && apt-get install -y cron && rm -rf /var/lib/apt/lists/*

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ backend
COPY --from=backend-builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=backend-builder /app/backend /app/backend

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ frontend
COPY --from=frontend-builder /app/frontend/dist /app/frontend/dist

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ cron –∑–∞–¥–∞—á
COPY cron/press-monitor-cron /etc/cron.d/press-monitor-cron
RUN chmod 0644 /etc/cron.d/press-monitor-cron
RUN crontab /etc/cron.d/press-monitor-cron

WORKDIR /app/backend

# –ó–∞–ø—É—Å–∫ cron –∏ —Å–µ—Ä–≤–µ—Ä–∞
CMD cron && langgraph serve --host 0.0.0.0 --port 8000
```

### 6.2 Cron –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

```bash
# cron/press-monitor-cron
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
0 */6 * * * cd /app/backend && python -m agent.scheduled_monitor all_languages >> /var/log/press-monitor.log 2>&1

# –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –≤ 9:00 UTC
0 9 * * * cd /app/backend && python -m agent.send_daily_digest >> /var/log/daily-digest.log 2>&1

# –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞–º
0 10 * * 1 cd /app/backend && python -m agent.send_weekly_report >> /var/log/weekly-report.log 2>&1
```

### 6.3 –°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

```python
# backend/src/agent/scheduled_monitor.py
import asyncio
import sys
from datetime import datetime
import logging
from typing import Optional

from .orchestrator import create_orchestrator_graph
from .notification_sender import send_email_digest, send_telegram_digest
from .database import get_email_subscribers, get_telegram_subscribers

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def run_scheduled_monitoring(mode: str = "all_languages"):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–ª–∞–Ω–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
    
    logger.info(f"Starting scheduled monitoring in {mode} mode at {datetime.now()}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ
        graph = create_orchestrator_graph()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        config = {
            "configurable": {
                "search_mode": mode,
                "translation_enabled": True,
                "max_articles_per_language": 10,
                "notification_enabled": True
            }
        }
        
        result = await graph.ainvoke(
            {"messages": [HumanMessage(content="Scheduled press monitoring")]},
            config=config
        )
        
        logger.info(f"Monitoring completed. Found {len(result['all_articles'])} articles")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —É–±—Ä–∞–Ω—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ frontend
        
    except Exception as e:
        logger.error(f"Error during scheduled monitoring: {e}")
        raise

def extract_statistics(result: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    return {
        "total_articles": len(result.get("all_articles", [])),
        "positive_count": len(result.get("positive_articles", [])),
        "negative_count": len(result.get("negative_articles", [])),
        "neutral_count": len(result.get("neutral_articles", [])),
        "languages_covered": len(set(a["source_language"] for a in result.get("all_articles", []))),
        "countries_covered": len(set(a["source_country"] for a in result.get("all_articles", [])))
    }

if __name__ == "__main__":
    mode = sys.argv[1] if len(sys.argv) > 1 else "all_languages"
    asyncio.run(run_scheduled_monitoring(mode))
```

### 6.4 Frontend API –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
# backend/src/agent/api_endpoints.py
from fastapi import APIRouter, Query, HTTPException
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta

router = APIRouter(prefix="/api/press-monitor")

@router.get("/search")
async def search_press(
    country: Optional[str] = None,
    language: Optional[str] = None,
    days_back: int = Query(7, ge=1, le=90),
    sentiment: Optional[str] = None
):
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"""
    
    articles = await get_articles_from_db(
        country=country,
        language=language,
        days_back=days_back,
        sentiment=sentiment
    )
    
    return {
        "articles": articles,
        "total": len(articles),
        "parameters": {
            "country": country,
            "language": language,
            "days_back": days_back,
            "sentiment": sentiment
        }
    }

@router.get("/temporal/{country}")
async def get_temporal_analysis(
    country: str,
    periods: List[int] = Query([7, 30, 90])
):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å—Ç—Ä–∞–Ω—ã"""
    
    agent = TemporalAnalyticsAgent()
    analysis = await agent.analyze_temporal_changes(
        country=country,
        comparison_periods=periods
    )
    
    return analysis

@router.get("/live-monitor")
async def start_live_monitoring(
    mode: str = Query("all_languages"),
    languages: Optional[List[str]] = None,
    regions: Optional[List[str]] = None
):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    result = await press_monitor_graph.ainvoke(
        {"messages": [HumanMessage(content=f"Monitor {mode}")]},
        config={
            "configurable": {
                "search_mode": mode,
                "target_languages": languages,
                "target_regions": regions
            }
        }
    )
    
    return {
        "status": "completed",
        "statistics": extract_statistics(result),
        "positive_digest": result.get("positive_digest"),
        "negative_digest": result.get("negative_digest"),
        "temporal_analyses": result.get("temporal_analyses")
    }
```

## üìã –ò–¢–û–ì–û–í–´–ô –ß–ï–ö–õ–ò–°–¢ –†–ï–§–ê–ö–¢–û–†–ò–ù–ì–ê

### ‚úÖ –§–∞–∑–∞ 1: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (–î–µ–Ω—å 1-2)
- [ ] –°–æ–∑–¥–∞—Ç—å SQL –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü
- [ ] –î–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [ ] –ó–∞–ø–æ–ª–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —è–∑—ã–∫–æ–≤
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å connection pooling

### ‚úÖ –§–∞–∑–∞ 2: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ State (–î–µ–Ω—å 3-4)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å state.py —Å –Ω–æ–≤—ã–º–∏ —Ç–∏–ø–∞–º–∏
- [ ] –†–∞—Å—à–∏—Ä–∏—Ç—å tools_and_schemas.py
- [ ] –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
- [ ] –°–æ–∑–¥–∞—Ç—å helper —Ñ—É–Ω–∫—Ü–∏–∏

### ‚úÖ –§–∞–∑–∞ 3: –ù–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã (–î–µ–Ω—å 5-7)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å orchestrator.py
- [ ] –°–æ–∑–¥–∞—Ç—å language_agents.py
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å sentiment_analyzer.py
- [ ] –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å digest_generator.py
- [ ] –°–æ–∑–¥–∞—Ç—å temporal_analytics.py –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã

### ‚úÖ –§–∞–∑–∞ 4: Frontend (–î–µ–Ω—å 8-9)
- [ ] –°–æ–∑–¥–∞—Ç—å PressMonitor –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
- [ ] –î–æ–±–∞–≤–∏—Ç—å DigestView
- [ ] –°–æ–∑–¥–∞—Ç—å TemporalAnalytics –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤
- [ ] –û–±–Ω–æ–≤–∏—Ç—å —Ä–æ—É—Ç–∏–Ω–≥
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å API
- [ ] –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π

### ‚úÖ –§–∞–∑–∞ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–î–µ–Ω—å 10-12)
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å unit —Ç–µ—Å—Ç—ã
- [ ] –°–æ–∑–¥–∞—Ç—å integration —Ç–µ—Å—Ç—ã
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### ‚úÖ –§–∞–∑–∞ 6: –î–µ–ø–ª–æ–π (–î–µ–Ω—å 13-14)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å Docker –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å cron –∑–∞–¥–∞—á–∏
- [ ] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å API

## üéØ –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò –£–°–ü–ï–•–ê

1. **–ü–æ–∫—Ä—ã—Ç–∏–µ —è–∑—ã–∫–æ–≤**: 60+ —è–∑—ã–∫–æ–≤ –º–∏—Ä–∞
2. **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏**: < 5 –º–∏–Ω—É—Ç –Ω–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
3. **–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**: > 85% –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
4. **–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã**: 99.9% uptime
5. **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö**: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤

## üîí –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨ –ò –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú–û–°–¢–¨

1. **Rate Limiting**: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Google API
2. **–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: Redis –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
3. **–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ multiple workers
4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: Prometheus + Grafana
5. **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**: Structured logging —Å correlation IDs

## üéØ –ò–¢–û–ì–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. **Multi-Language Press Monitor** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–µ—Å—Å—ã –Ω–∞ 60+ —è–∑—ã–∫–∞—Ö –º–∏—Ä–∞
2. **Sentiment Analyzer** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π
3. **Temporal Analytics Agent** - –∞–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
4. **Interactive Frontend** - React –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
5. **PostgreSQL + Redis** - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- ‚úÖ –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞ –Ω–∞ –í–°–ï–• —è–∑—ã–∫–∞—Ö –º–∏—Ä–∞
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
- ‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- ‚úÖ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- ‚úÖ API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
- ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

### –û—Å–æ–±—ã–π —Ñ–æ–∫—É—Å –Ω–∞:
- üéØ –°–æ—Å–µ–¥–∏ –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞ (–¢—É—Ä—Ü–∏—è, –†–æ—Å—Å–∏—è, –ò—Ä–∞–Ω, –ì—Ä—É–∑–∏—è, –ê—Ä–º–µ–Ω–∏—è)
- üéØ –í—Å—è –°—Ä–µ–¥–Ω—è—è –ê–∑–∏—è
- üéØ –Æ–≥–æ-–í–æ—Å—Ç–æ—á–Ω–∞—è –ê–∑–∏—è (–¢–∞–∏–ª–∞–Ω–¥, –ò–Ω–¥–æ–Ω–µ–∑–∏—è –∏ –¥—Ä.)
- üéØ –í–°–ï —Ä–µ–≥–∏–æ–Ω—ã –º–∏—Ä–∞ –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏—è

–≠—Ç–æ—Ç –ø–ª–∞–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–ª–∞–≤–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –≤ –º–æ—â–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–∏—Ä–æ–≤–æ–π –ø—Ä–µ—Å—Å—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ–π —Ä–∞–±–æ—Ç–∞—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∞—Ö.